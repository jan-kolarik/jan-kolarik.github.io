[{"content":"Intro In this post, I\u0026rsquo;d like to share some insights on the crucial topic of maintaining ABI compatibility with programs written in C++, especially when your component serves as a dependency in a software ecosystem. I\u0026rsquo;ll try to provide some tips and insights into our approach within the DNF5 project and explore specific tools for analyzing issues related to ABI changes.\nUnderstanding ABI compatibility In simple terms, the Application Binary Interface (ABI) is a contract between different parts of a software system which ensures they can work together seamlessly, regardless of the programming languages or compilers used to create them.\nWhen we mention maintaining ABI compatibility, our goal is to ensure that clients of our library do not need to recompile their applications when downloading and deploying a new version. They should be able to continue running their application builds without making any changes themselves.\nABI vs API It\u0026rsquo;s also important to distinguish between ABI compatibility and API compatibility.\nAPI compatibility focuses on preserving the functionality and syntax of the exposed interfaces. In other words, ABI compatibility ensures that the compiled code can work interchangeably, while API compatibility guarantees that the expected methods, parameters, and behaviors remain consistent across different versions of the library.\nBoth aspects are crucial for a smooth and easy integration of new library versions into existing applications.\nHow to not break anything To ensure ABI compatibility, developers should adhere to best practices, which include avoiding changes to the layout of classes, the size of data types, and the order of virtual functions in interfaces.\nThe KDE community has compiled a comprehensive guide on binary compatibility issues with C++ that you might find helpful. The guide provides a list of do’s and don’ts when writing cross-platform C++ code meant to be compiled with several different compilers.\nWhile breaking ABI compatibility is generally undesirable, there are situations where it becomes necessary. This may be due to the need to address critical security vulnerabilities, eliminate long-deprecated features that impede development progress, or undertake significant refactoring to introduce major enhancements that require incompatible changes in the library.\nOur approach to ABI compatibility in DNF5 Pimpl In DNF5, we commonly face a scenario during development where adding new items to existing structures or classes, part of the external interface, can potentially break ABI compatibility. To mitigate this, we proactively identify candidates that might be affected in the future and convert them to use the Pimpl idiom.\nThe Pimpl idiom involves hiding the implementation details of a class behind a pointer, using a forward declaration in the header file, and keeping the implementation details in a separate source file. The public interface exposed in the header file remains stable since it only contains a forward declaration of the implementation class, ensuring that changes to internal implementation details won\u0026rsquo;t affect the ABI as seen by external code.\nBranching Additionally, when anticipating an ABI-breaking change, we create a separate development branch for introducing the next major version release. Here, we consolidate all planned breaking changes, conduct thorough testing, and introduce the changes in a single release. This approach not only minimizes disruption for users but also allows more efficient testing of the entire set of changes and enables clear communication with users about planned modifications.\nBumping soname Ensuring seamless transitions during changes that break the ABI, we practice soname library bumps. This involves incrementing the version number associated with our shared libraries on Linux systems. By doing so, we signal to users and applications that a backward-incompatible change has occurred. This soname bump not only aligns with semantic versioning principles but also helps with symbol versioning and dependency management.\nWhat about some tools Of course, there are a lot of tools for static analysis that can compare the new and existing binaries and check for backward API/ABI compatibility.\nKeep in mind that any tool may produce false positives, so a manual review of the outputs is still required.\nABI Compliance Checker One well-known tool for this purpose is the ABI Compliance Checker, capable of generating clear XML or HTML outputs:\nFor a quick guide on using the commands, refer to this resource.\nabidiff Another commonly used tool is abidiff, a command-line utility that compares the ABI of two ELF shared libraries. It generates textual reports detailing changes affecting exported functions, variables, and their types.\nIt can be easily deployed as a GitHub action to check ABI compatibility with new changes in submitted pull requests. For example, you can review the mlibc project, which has the configuration for such a workflow here.\nExploring Fedora Linux Here are some specific insights related to the Fedora Linux and its RPM packages ecosystem.\nrpminspect When a package maintainer submits a new build or update through the Bodhi system, automated test suites evaluate the build candidate. Some tests are mandatory, preventing the package submission if they fail, while others are optional, requiring user verification for potential issues:\nOne such tool in these automated tests is rpminspect, contributing to general analysis of RPM packages. It produces a comprehensive report on policy compliance, changes between the previous and current build, and overall correctness and best practices. Abidiff is utilized there as one of the available analysis modes:\nPackit and Testing Farm The Packit project, designed to automate the package release process, has recently gained significant popularity. Maintainers can effortlessly create configurations based on a wide range of available examples. Packit automation streamlines the entire release process, from triggering builds and manipulating package spec files to generating changelogs and running tests.\nIn the context of checking ABI changes, there\u0026rsquo;s a simple way to configure a GitHub action in your upstream project, enabling the triggering of rpminspect analysis on pull requests. This functionality is managed by the Testing Farm, Packit\u0026rsquo;s testing system, which is an integral part of the Fedora CI infrastructure.\nReferences KDE Guide to Binary Compatibility ABI Compliance Checker abidiff rpminspect ","permalink":"https://jan-kolarik.github.io/posts/abi-compatibility/","summary":"Update the codebase safely to not break your consumers","title":"Maintaining C++ ABI compatibility"},{"content":"Three weeks ago, I presented at our internal Red Hat event, QEcamp. This event serves as a platform for discussing all aspects related to quality and testing. What makes it exceptional is its inclusivity — it brings together not just quality engineers but associates from various departments, fostering discussions around the topic of quality.\nMy contribution included an overview presentation on test-driven development, where I shared the basics and my experience with this alternative approach to the conventional development process. You can view the video recording here if you are interested. HTML slides are available here.\n","permalink":"https://jan-kolarik.github.io/posts/tdd-qecamp23/","summary":"An introductory presentation about test-driven development","title":"TDD talk at QEcamp23"},{"content":"Intro A common use case for DNF5 API is installing packages, which is quite straightfoward.\nFirst, we need to create a Base, the core object that holds a runtime environment. We load its configuration from the system and run the setup method to prepare the environment.\nNext, we can prepare a repository sack, which holds information about configured repositories and the state of local and remote packages, while potentially refreshing metadata from remote servers if needed.\nWith the setup complete, we can tell DNF5 what we want to do, in this case, install our package. This is done by configuring the Goal object.\nAfter defining our intention, we can proceed to calculate the transaction, determining the necessary actions to achieve our goal. Finally, we can perform the resulting action, which is downloading and installing the packages.\nAn example Python script that accomplishes this might look like this:\nimport libdnf5 base = libdnf5.base.Base() base.load_config_from_file() base.setup() sack = base.get_repo_sack() sack.create_repos_from_system_configuration() sack.update_and_load_enabled_repos(True) goal = libdnf5.base.Goal(base) goal.add_install(\u0026#39;my-awesome-package\u0026#39;) transaction = goal.resolve() transaction.download() transaction.run() After this point, if everything went well, the package should be successfully installed on the system.\nWhat could be tricky is when trying to use the API after the transaction. The problem is that the existing repository sack does not reflect the updated state after the transaction was executed. This is because managing that state with connected third-party libraries would be very difficult.\nUse information from the Transaction object If you only need to query information about the post-transaction state, you can use the data provided by the Transaction object.\nThe get_transaction_packages() method can be particularly useful for this purpose. It allows us to query which packages were involved in the transaction, the specific actions taken with these packages, and any packages they may have been replaced with.\nFor example, let\u0026rsquo;s say we want to retrieve a list of new files that were installed during the transaction. In this case, we\u0026rsquo;ll need to pre-load the filelists metadata in DNF5 before loading the repository sack. We can do this by adding the following code:\nbase.get_config().get_optional_metadata_types_option().add_item(\u0026#39;filelists\u0026#39;) Then, you can use the helper function transaction_item_action_is_inbound to filter only inbound packages from the transaction. Finally, you can query the package files contained in the transaction:\nnewly_installed_files = set() for transaction_package in transaction.get_transaction_packages(): action = transaction_package.get_action() if libdnf5.base.transaction.transaction_item_action_is_inbound(action): package_files = transaction_package.get_package().get_files() newly_installed_files |= set(package_files) Start with a new Base The easiest and most robust way is probably to create a new Base each time we need a fresh state. Although some work is done repeatedly, there is no additional unnecessary network trafic as the metadata is already refreshed during the first attempt. This approach allows us to perform any task as if we were executing a new separate script for each one.\nimport libdnf5 def create_base(): base = libdnf5.base.Base() base.load_config_from_file() base.setup() sack = base.get_repo_sack() sack.create_repos_from_system_configuration() sack.update_and_load_enabled_repos(True) return base def install_package(spec): base = create_base() goal = libdnf5.base.Goal(base) goal.add_install(spec) transaction = goal.resolve() transaction.download() transaction.run() def query_installed(): base = create_base() query = libdnf5.rpm.PackageQuery(base) query.filter_installed() return [package.get_nevra() for package in query] install_package(\u0026#39;my-awesome-package\u0026#39;) print(query_installed()) Use the RPM API Another alternative is to use the underlying RPM API. This should be the most effective way for querying information about installed packages, as we are directly reading the data from the SQLite database:\nimport rpm # Prepare the transaction set while ignoring package signatures verification transaction_set = rpm.TransactionSet() transaction_set.setVSFlags(rpm._RPMVSF_NOSIGNATURES) # Find the newest package in the database last_package = max(transaction_set.dbMatch(), key=lambda package: package[rpm.RPMTAG_INSTALLTIME]) # Get packages only related to the latest transaction last_packages = transaction_set.dbMatch(rpm.RPMTAG_INSTALLTID, last_package[rpm.RPMTAG_INSTALLTID]) # Aggregate all related files files = set() for package in last_packages: files |= set(package[rpm.RPMTAG_FILENAMES]) References DNF5 upstream RPM upstream ","permalink":"https://jan-kolarik.github.io/posts/dnf5-api-after-transaction/","summary":"What happens to the existing sack and how to deal with that","title":"Using DNF5 API after running a transaction"},{"content":" Intro On a bit different note, recently I was trying to help my father with adding debugging support for Arduino Mega 2560 microcontroller board without need to use any additional hardware kit.\nFor Windows, there is a great tutorial for that, but we need to setup several things differently in the Linux environment and I didn\u0026rsquo;t find any easy-to-use guide for that.\nSo I want to share my approach with you, definitely nothing world-shattering, but could be useful for anyone with similar intention or might save you some time from being stuck in one place for too long.\nEnvironment Fedora Linux (but should be applicable to any other distro) Visual Studio Code + Arduino extension Arduino CLI avr_debug Building and uploading First we need to setup the building of our project and make it possible to upload the binary into the board.\nPrepare VS Code Install the Arduino CLI tool to provide an all-in-one solution for Arduino boards.\nThen we need to install \u0026amp; enable the Arduino extension in the VS Code. In the extension\u0026rsquo;s configuration we need to modify some fields:\nArduino: Command Path (arduino.commandPath) \u0026ndash; set arduino-cli Arduino: Path (arduino.path) \u0026ndash; set /path/to/your/arduino-cli/binary Arduino: Use Arduino Cli (arduino.useArduinoCli) \u0026ndash; set true Initialize the Arduino project with the Arduino: Initialize command, f.e. by selecting it from the command palette using the F1 key in VS Code. This will generate the empty .ino source file and the default arduino.json extension config for our project in the .vscode folder. You will also need to rename this existing .ino file to match the name of the project.\nOpen the Arduino: Board Manager command and install the Arduino AVR Boards package to add support for our microcontroller board.\nUsing the bottom status bar in VS Code, select the Arduino Mega board type.\nNow you can try to compile the empty project using the Arduino: Verify command which is also available under the icon in the editor\u0026rsquo;s top bar.\nDeploy a simple program To be able to upload our program to the actual board, it might be needed to add appropriate user rights by adding the current user to the dialout or tty group:\nsudo usermod -a -G dialout $USER It will need a logout or restart.\nConnect the board to the USB and select the /dev/ttyUSB0 port from the bottom status bar.\nWe should be able now to upload the program to the board with the Arduino: Upload command.\nTo see if it is really working you can add some simple blinking to the .ino source:\nvoid setup() { pinMode(LED_BUILTIN, OUTPUT); } void loop() { digitalWrite(LED_BUILTIN, HIGH); delay(1000); digitalWrite(LED_BUILTIN, LOW); delay(1000); } Debugging Here we\u0026rsquo;ll add support for debugging our board from the VS Code using just the connected USB cable.\nSetup the debugger We will prepare the GDB debugger that will be running on our local computer and the avr_debug, remote stub which will be deployed to the ATMega and communicating with the GDB to allow debugging from VS Code.\nUnfortunately, for Fedora Linux, there is no maintained package with GDB for AVR architectures anymore, therefore we need to build it by ourselves. Luckily it\u0026rsquo;s very simple.\nDownload the latest GDB sources, compile it for target AVR architecture and install it in the defined directory like this:\n./configure --target=avr --prefix=${HOME}/Programs/avr-gdb make make install Then unpack the avr_debug library into the Arduino user libraries directory at ${HOME}/Arduino/libraries/avr-debugger. Put there just the library sub-directory from the default branch.\nConfigure the project Include the debugging library into our project by going to the Arduino: Library Manager, filtering the avr-debugger and clicking the Include Library button.\nNow we have the sources prepared for adding the debugging support which is as simple as adding the debug_init() function at the beginning of the setup():\n#include \u0026lt;app_api.h\u0026gt; #include \u0026lt;avr_debugger.h\u0026gt; #include \u0026lt;avr8-stub.h\u0026gt; void setup() { debug_init(); pinMode(LED_BUILTIN, OUTPUT); } void loop() { digitalWrite(LED_BUILTIN, HIGH); delay(1000); digitalWrite(LED_BUILTIN, LOW); delay(1000); } Last thing we need to do is to create a debugging configuration for VS Code, so it knows what we want to debug and how.\nI am sharing my launch.json example below. The key parts there:\nprogram points to our built program which is the .elf binary in the build folder miDebuggerPath addresses the AVR GDB binary we have built setupCommands section tells GDB what port and baud rate to use { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Debugger launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/example.ino.elf\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, \u0026#34;externalConsole\u0026#34;: true, \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, \u0026#34;miDebuggerPath\u0026#34;: \u0026#34;${userHome}/Programs/avr-gdb/bin/avr-gdb\u0026#34;, \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Set remote serial baud\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;set serial baud 115200\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: false }, { \u0026#34;description\u0026#34;: \u0026#34;Attach to serial port\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;target remote /dev/ttyUSB0\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: false } ] } ] } You can put it in the .vscode subfolder and modify it according to your environment.\nLet\u0026rsquo;s add some breakpoints! And so now we can verify and upload our program and then debug the running program in VS Code as with any other project. Just setup the breakpoints and then attach to the board using the Run and Debug from the Activity Bar.\nNote: there is one important downside of this approach and it\u0026rsquo;s that no Serial functions could be used when the debugging stub is attached. You could use the conditional compilation to enable them when not debugging. For more info, refer to the upstream project of the avr_debug author.\n","permalink":"https://jan-kolarik.github.io/posts/atmega-debug-linux/","summary":"Using Visual Studio Code to debug programs for Arduino Mega 2560","title":"Debugging Arduino without additional hardware"},{"content":"Intro If you are planning to make a presentation including some live command-line examples, the following article could be useful for you.\nI\u0026rsquo;d like to share with you my setup which results in HTML presentation containing embedded console windows which can run isolated from your host system.\nPrerequisites Following technologies are used to create the resulting presentation:\nreveal.js - framework for creating HTML presentations ttyd - tool to access Linux shell over HTTP podman - tool for managing containers Show the code first Prepare the example Let\u0026rsquo;s say we want to present and describe an example of our source code and then show the audience how it is being run on the target system using the CLI.\nHere we\u0026rsquo;ll use this simple Python snippet:\nimport random # Generate the number number = random.randint(1, 100) # Print the number print(f\u0026#39;Your lucky number is {number}.\u0026#39;) Insert it inside the presentation Suppose we have already configured and running some instance of the reveal.js presentation, we can insert a section with our code example there:\n\u0026lt;section\u0026gt; \u0026lt;h4\u0026gt;Python demo\u0026lt;/h4\u0026gt; \u0026lt;pre\u0026gt; \u0026lt;code\u0026gt; import random # Generate the number number = random.randint(1, 100) # Print the number print(f\u0026#39;Your lucky number is {number}.\u0026#39;) \u0026lt;/code\u0026gt; \u0026lt;/pre\u0026gt; \u0026lt;/section\u0026gt; Stylize the snippet Now we can play a bit more with the layout. We could change the default font-size and the width of the code block, so our snippet doesn\u0026rsquo;t contain any scrollbars and it is better centered within the presentation screen.\nBy default the reveal.js presentation has configured the highlight.js plugin for syntax highlighting, so we can define the language of our snippet and apply the colors by adding the class=\u0026quot;hljs language-python\u0026quot;.\nTo emphasize only part of the code step by step, we can use the data-line-numbers attribute where the vertical bar character denotes the transitions, f.e. \u0026quot;|1|3-4|6-7\u0026quot; means starting with the whole code highlighted, followed by just line number 1, then lines 3-4 and ending with lines 6-7.\nThe result could look like this:\n\u0026lt;section\u0026gt; \u0026lt;h4\u0026gt;Python demo\u0026lt;/h4\u0026gt; \u0026lt;pre style=\u0026#34;font-size: 18px; width: 60%;\u0026#34;\u0026gt; \u0026lt;code class=\u0026#34;hljs language-python\u0026#34; data-line-numbers=\u0026#34;|1|3-4|6-7\u0026#34;\u0026gt; import random # Generate the number number = random.randint(1, 100) # Print the number print(f\u0026#39;Your lucky number is {number}.\u0026#39;) \u0026lt;/code\u0026gt; \u0026lt;/pre\u0026gt; \u0026lt;/section\u0026gt; Add an interactive console Setup the web terminal Deploying the shell web server is very simple. When we have downloaded the ttyd binary, we just provide the port number where the daemon will be listening and providing the HTTP layer above the console.\nFollowing example will deploy ttyd web server on the port 1234 and for every connected client it will create a new process with bash:\nttyd -p 1234 bash Integrate the console Putting the console into the presentation is as simple as adding new iframe pointing to our ttyd service at http://localhost:1234/:\n\u0026lt;iframe src=\u0026#34;http://localhost:1234/\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; Tuning the visual Now to create a seamless transition between the code snippet and the console window we need to add a bit more configuration.\nWe can setup a custom font size and also change the colors for ttyd console like this:\nttyd -p 1234 -t fontSize=12 -t \u0026#39;theme={\u0026#34;background\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;foreground\u0026#34;: \u0026#34;black\u0026#34;}\u0026#39; bash In reveal.js we will stack the console frame window on the top of the code snippet while keeping it invisible until the snippet code slides are fully traversed. This could be done by including the both frames inside the parent div having the r-stack class and showing the console at the right moment by adding the fragment class to the console iframe.\nIn the end we can change the console frame size to match the code snippet.\nOne hack that could be handy when we don\u0026rsquo;t want to show the vertical scrollbar inside the console frame, but still keeping the scrolling functionality. In this case we can wrap the console in the div which will match the size of the code example frame, but we stretch the width of the actual iframe a bit, so the scrollbar is hidden. This also needs to setup overflow: hidden; in the wrapping div.\nSo the result could look like this:\n\u0026lt;section\u0026gt; \u0026lt;h4\u0026gt;Python demo\u0026lt;/h4\u0026gt; \u0026lt;div class=\u0026#34;r-stack\u0026#34;\u0026gt; \u0026lt;pre id=\u0026#34;code\u0026#34;\u0026gt; \u0026lt;code class=\u0026#34;hljs language-python\u0026#34; data-trim data-line-numbers=\u0026#34;|1|3-4|6-7\u0026#34;\u0026gt; import random # Generate the number number = random.randint(1, 100) # Print the number print(f\u0026#39;Your lucky number is {number}.\u0026#39;) \u0026lt;/code\u0026gt; \u0026lt;/pre\u0026gt; \u0026lt;div id=\u0026#34;cli-wrapper\u0026#34;\u0026gt; \u0026lt;iframe id=\u0026#34;cli\u0026#34; class=\u0026#34;fragment fade-up\u0026#34; src=\u0026#34;http://localhost:1234/\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; And the CSS now moved into it\u0026rsquo;s own stylesheet:\n#code { font-size: 18px; width: 60%; } #cli-wrapper { max-width: 60%; width: 60%; max-height: 100%; height: 100%; overflow: hidden; } #cli { max-width: 103%; width: 103%; max-height: 100%; height: 100%; } Note: I am definitely not a CSS guy, so please don\u0026rsquo;t blame me if you find anything ridiculous about the mentioned code. But, it should work 😇\nThis is the final output when running the presentation in the web browser:\nThere should have been a video here but your browser does not seem to support it. Using containers When doing more examples in the presentation it might be useful to always have an isolated environment for each demo.\nThis can be done easily by using the podman containers. We can deploy a container from the public image, do some customizations, prepare our demo environment and then serialize the state of the container. Then we can setup ttyd to run a clean container from this image every time client requests new console.\nSo if we use the Fedora Linux as an example, we can download the latest Fedora container image and get inside that:\npodman pull fedora podman run -it fedora It will redirect us inside the container terminal:\n[root@fdee00b17d43 /]# Now prepare the environment needed for the demo, like installing dependencies, copying the example scripts into the container etc.\nWhen everything is ready, we can export the container from another shell:\npodman container export fdee00b17d43 \u0026gt; container.tar Then we can import it in the image registry on local or any other computer and tagging it with example-container name by doing:\npodman image import container.tar example-container Finally we will prepare the ttyd daemon to spawn a new container for us on each attach:\nttyd -p 1234 podman run -it example-container /bin/bash We can also change the container\u0026rsquo;s hostname with -h my-hostname, so the shell on the live demo will not show the ugly auto-generated id.\nAnd of course we can prepare many containers running on different ports with various font sizes, configuration, etc.\nNote: each time client connects to the ttyd, new container is created. This also means when the page having the embedded terminal is refreshed. Therefore it may be desirable to cleanup all the related containers after the presentation is done:\npodman container rm --filter ancestor=localhost/example-container References Here are links to show you an example of such a presentation. It is from our DNF5 talk we had at FOSDEM last week:\nSources Slides Recording ","permalink":"https://jan-kolarik.github.io/posts/terminal-presentation/","summary":"How to setup a seamless interactive console within your presentation","title":"Creating slides with integrated shell"},{"content":"My name is Jan Kolárik and I\u0026rsquo;m a software engineer currently working in Software Management team on stuff related to DNF package manager in Red Hat.\nI first discovered Linux when I was 14 years old and a few years later I started using it as my main operating system.\nRecently, I\u0026rsquo;ve entered the world of open source as a developer, which has been my dream for a long time.\nI have experience mainly with C++, C# and Python languages and I love applying test-driven development principles in my projects wherever it\u0026rsquo;s possible. Currently I am also discovering the beauties of Rust and delving into Kubernetes technology.\n","permalink":"https://jan-kolarik.github.io/about/","summary":"About me","title":"About"},{"content":" Last weekend I\u0026rsquo;ve attended FOSDEM 2023 conference with my colleagues from the Red Hat RPM software management team. We gave a talk about the DNF5 package manager which will be the new default in Fedora Linux soon.\nIt was the first time for me speaking in public at such a huge event and it was really exciting. This whole meetup was very energizing. Lot of different positive vibes from various tracks. I hope I\u0026rsquo;ll make it there again next year.\nIf you are more interested, you can visit this link and see a recording of our presentation or any other talk from the conference.\n","permalink":"https://jan-kolarik.github.io/posts/dnf5-fosdem2023/","summary":"Presenting the upcoming Fedora Linux package manager in Brussels","title":"DNF5 at FOSDEM"}]